{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737e5560-d4b9-4f8b-8234-2b4d641b123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class WeatherData:\n",
    "    \"\"\"Structure for weather data\"\"\"\n",
    "    timestamp: datetime\n",
    "    wind_u: float\n",
    "    wind_v: float\n",
    "    pressure: float\n",
    "    temperature: float\n",
    "    wind_speed: float\n",
    "    wind_direction: float\n",
    "    hour: int\n",
    "    day_of_year: int\n",
    "\n",
    "@dataclass\n",
    "class StormSurgePrediction:\n",
    "    \"\"\"Structure for storm surge predictions\"\"\"\n",
    "    prediction_time: datetime\n",
    "    surge_height: float\n",
    "    alert_level: str\n",
    "    confidence: float\n",
    "    location: str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f61148-3f52-4604-95e6-b5a42fc57892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeWeatherAPI:\n",
    "    \"\"\"Handles multiple weather API sources with fallbacks\"\"\"\n",
    "    \n",
    "    def __init__(self, openweather_api_key: Optional[str] = None):\n",
    "        self.openweather_api_key = openweather_api_key\n",
    "        self.session = requests.Session()\n",
    "        self.session.timeout = 10\n",
    "        \n",
    "    def get_weather_data(self, latitude: float, longitude: float, \n",
    "                        hours_back: int = 24) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get weather data with multiple API fallbacks\n",
    "        \n",
    "        Args:\n",
    "            latitude: Location latitude\n",
    "            longitude: Location longitude\n",
    "            hours_back: Hours of historical data needed\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with weather data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try Open-Meteo first (free, reliable)\n",
    "            logger.info(\"Fetching weather data from Open-Meteo...\")\n",
    "            return self._get_open_meteo_data(latitude, longitude, hours_back)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Open-Meteo failed: {e}\")\n",
    "            \n",
    "            # Fallback to OpenWeatherMap if API key available\n",
    "            if self.openweather_api_key:\n",
    "                try:\n",
    "                    logger.info(\"Trying OpenWeatherMap as fallback...\")\n",
    "                    return self._get_openweather_data(latitude, longitude, hours_back)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"OpenWeatherMap failed: {e}\")\n",
    "            \n",
    "            # Final fallback to synthetic current data\n",
    "            logger.warning(\"Using synthetic current weather data\")\n",
    "            return self._generate_current_weather_fallback(latitude, longitude, hours_back)\n",
    "    \n",
    "    def _get_open_meteo_data(self, latitude: float, longitude: float, \n",
    "                           hours_back: int) -> pd.DataFrame:\n",
    "        \"\"\"Get data from Open-Meteo API (free)\"\"\"\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        start_time = end_time - timedelta(hours=hours_back)\n",
    "        \n",
    "        # Open-Meteo Historical Weather API\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"start_date\": start_time.strftime(\"%Y-%m-%d\"),\n",
    "            \"end_date\": end_time.strftime(\"%Y-%m-%d\"),\n",
    "            \"hourly\": [\n",
    "                \"temperature_2m\",\n",
    "                \"surface_pressure\",\n",
    "                \"wind_speed_10m\",\n",
    "                \"wind_direction_10m\",\n",
    "                \"wind_gusts_10m\"\n",
    "            ],\n",
    "            \"timezone\": \"UTC\"\n",
    "        }\n",
    "        \n",
    "        response = self.session.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'hourly' not in data:\n",
    "            raise ValueError(f\"Invalid API response: {data}\")\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'time': pd.to_datetime(data['hourly']['time']),\n",
    "            'temperature': data['hourly']['temperature_2m'],\n",
    "            'pressure': [p * 100 if p else None for p in data['hourly']['surface_pressure']],\n",
    "            'wind_speed': data['hourly']['wind_speed_10m'],\n",
    "            'wind_direction': data['hourly']['wind_direction_10m']\n",
    "        })\n",
    "        \n",
    "        return self._process_weather_dataframe(df)\n",
    "    \n",
    "    def _get_openweather_data(self, latitude: float, longitude: float, \n",
    "                            hours_back: int) -> pd.DataFrame:\n",
    "        \"\"\"Get data from OpenWeatherMap API (requires API key)\"\"\"\n",
    "        \n",
    "        # Get current weather\n",
    "        current_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "        current_params = {\n",
    "            \"lat\": latitude,\n",
    "            \"lon\": longitude,\n",
    "            \"appid\": self.openweather_api_key,\n",
    "            \"units\": \"metric\"\n",
    "        }\n",
    "        \n",
    "        current_response = self.session.get(current_url, params=current_params)\n",
    "        current_response.raise_for_status()\n",
    "        current_data = current_response.json()\n",
    "        \n",
    "        # Get historical data (limited to 5 days)\n",
    "        historical_data = []\n",
    "        for i in range(min(5, hours_back // 24 + 1)):\n",
    "            timestamp = int((datetime.now() - timedelta(days=i)).timestamp())\n",
    "            \n",
    "            hist_url = \"http://api.openweathermap.org/data/2.5/onecall/timemachine\"\n",
    "            hist_params = {\n",
    "                \"lat\": latitude,\n",
    "                \"lon\": longitude,\n",
    "                \"dt\": timestamp,\n",
    "                \"appid\": self.openweather_api_key,\n",
    "                \"units\": \"metric\"\n",
    "            }\n",
    "            \n",
    "            hist_response = self.session.get(hist_url, params=hist_params)\n",
    "            if hist_response.status_code == 200:\n",
    "                hist_data = hist_response.json()\n",
    "                if 'hourly' in hist_data:\n",
    "                    historical_data.extend(hist_data['hourly'])\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        rows = []\n",
    "        for hour_data in historical_data[-hours_back:]:\n",
    "            rows.append({\n",
    "                'time': pd.to_datetime(hour_data['dt'], unit='s'),\n",
    "                'temperature': hour_data['temp'],\n",
    "                'pressure': hour_data['pressure'] * 100,  # hPa to Pa\n",
    "                'wind_speed': hour_data['wind_speed'],\n",
    "                'wind_direction': hour_data.get('wind_deg', 0)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        return self._process_weather_dataframe(df)\n",
    "    \n",
    "    def _generate_current_weather_fallback(self, latitude: float, longitude: float,\n",
    "                                         hours_back: int) -> pd.DataFrame:\n",
    "        \"\"\"Generate synthetic current weather as fallback\"\"\"\n",
    "        \n",
    "        logger.warning(\"Generating synthetic weather data as fallback\")\n",
    "        \n",
    "        # Generate realistic current weather based on location and season\n",
    "        current_time = datetime.now()\n",
    "        times = [current_time - timedelta(hours=i) for i in range(hours_back, 0, -1)]\n",
    "        \n",
    "        # Seasonal temperature (rough approximation by latitude)\n",
    "        base_temp = 15 + 10 * np.cos(2 * np.pi * current_time.timetuple().tm_yday / 365)\n",
    "        if latitude > 40:  # Northern regions\n",
    "            base_temp -= 5\n",
    "        elif latitude < 25:  # Tropical regions\n",
    "            base_temp += 10\n",
    "        \n",
    "        data = []\n",
    "        for i, time_point in enumerate(times):\n",
    "            # Add daily temperature cycle\n",
    "            hour_temp = base_temp + 8 * np.sin(2 * np.pi * (time_point.hour - 6) / 24)\n",
    "            temp = hour_temp + np.random.normal(0, 2)\n",
    "            \n",
    "            # Generate wind (coastal areas typically windier)\n",
    "            wind_speed = 5 + np.random.exponential(3)\n",
    "            wind_dir = np.random.uniform(0, 360)\n",
    "            \n",
    "            # Generate pressure (typical sea level pressure with variation)\n",
    "            pressure = 101325 + np.random.normal(0, 1000)\n",
    "            \n",
    "            data.append({\n",
    "                'time': time_point,\n",
    "                'temperature': temp,\n",
    "                'pressure': pressure,\n",
    "                'wind_speed': wind_speed,\n",
    "                'wind_direction': wind_dir\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return self._process_weather_dataframe(df)\n",
    "    \n",
    "    def _process_weather_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process and standardize weather DataFrame\"\"\"\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.interpolate().fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        # Convert temperature to Kelvin if needed\n",
    "        if df['temperature'].mean() < 100:  # Assume Celsius\n",
    "            df['temperature'] = df['temperature'] + 273.15\n",
    "        \n",
    "        # Calculate wind components\n",
    "        wind_rad = df['wind_direction'] * np.pi / 180\n",
    "        df['wind_u'] = -df['wind_speed'] * np.sin(wind_rad)\n",
    "        df['wind_v'] = -df['wind_speed'] * np.cos(wind_rad)\n",
    "        \n",
    "        # Add time features\n",
    "        df['hour'] = df['time'].dt.hour\n",
    "        df['day_of_year'] = df['time'].dt.dayofyear\n",
    "        \n",
    "        # Ensure column order matches training data\n",
    "        feature_columns = ['wind_u', 'wind_v', 'pressure', 'temperature', \n",
    "                          'wind_speed', 'wind_direction', 'hour', 'day_of_year']\n",
    "        \n",
    "        return df[['time'] + feature_columns].sort_values('time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305aaf1-6e01-4960-8bf0-09e317c9a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TideGaugeAPI:\n",
    "    \"\"\"Handles real-time tide gauge data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.timeout = 10\n",
    "    \n",
    "    def get_tide_data(self, station_id: str, country: str = \"US\", \n",
    "                     hours_back: int = 24) -> pd.DataFrame:\n",
    "        \"\"\"Get recent tide gauge observations\"\"\"\n",
    "        \n",
    "        if country.upper() == \"US\":\n",
    "            return self._get_noaa_data(station_id, hours_back)\n",
    "        else:\n",
    "            logger.warning(f\"Country {country} not implemented, using synthetic data\")\n",
    "            return self._generate_synthetic_tide_data(hours_back)\n",
    "    \n",
    "    def _get_noaa_data(self, station_id: str, hours_back: int) -> pd.DataFrame:\n",
    "        \"\"\"Get NOAA tide data\"\"\"\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        start_time = end_time - timedelta(hours=hours_back)\n",
    "        \n",
    "        url = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
    "        params = {\n",
    "            \"date\": f\"{start_time.strftime('%Y%m%d %H:%M')}-{end_time.strftime('%Y%m%d %H:%M')}\",\n",
    "            \"station\": station_id,\n",
    "            \"product\": \"water_level\",\n",
    "            \"datum\": \"MSL\",\n",
    "            \"time_zone\": \"GMT\",\n",
    "            \"units\": \"metric\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'error' in data:\n",
    "                raise ValueError(f\"NOAA API error: {data['error']}\")\n",
    "            \n",
    "            df = pd.DataFrame(data['data'])\n",
    "            df['time'] = pd.to_datetime(df['t'])\n",
    "            df['water_level'] = pd.to_numeric(df['v'], errors='coerce')\n",
    "            \n",
    "            return df[['time', 'water_level']].dropna()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to get NOAA data: {e}\")\n",
    "            return self._generate_synthetic_tide_data(hours_back)\n",
    "    \n",
    "    def _generate_synthetic_tide_data(self, hours_back: int) -> pd.DataFrame:\n",
    "        \"\"\"Generate synthetic tide data as fallback\"\"\"\n",
    "        \n",
    "        times = [datetime.now() - timedelta(hours=i) for i in range(hours_back, 0, -1)]\n",
    "        \n",
    "        # Generate realistic tidal pattern\n",
    "        water_levels = []\n",
    "        for time_point in times:\n",
    "            hours_since_epoch = time_point.timestamp() / 3600\n",
    "            # M2 tide (12.42 hour period)\n",
    "            m2_tide = 0.8 * np.cos(2 * np.pi * hours_since_epoch / 12.42)\n",
    "            # S2 tide (12 hour period)  \n",
    "            s2_tide = 0.3 * np.cos(2 * np.pi * hours_since_epoch / 12.0)\n",
    "            # Add some noise\n",
    "            noise = np.random.normal(0, 0.1)\n",
    "            \n",
    "            water_levels.append(m2_tide + s2_tide + noise)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'time': times,\n",
    "            'water_level': water_levels\n",
    "        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bcf8110-93cd-48d6-984b-c4321a8a84cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeSurgePredictor:\n",
    "    \"\"\"Real-time storm surge prediction system\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, location_name: str = \"Coastal Station\"):\n",
    "        self.location_name = location_name\n",
    "        self.weather_api = RealTimeWeatherAPI()\n",
    "        self.tide_api = TideGaugeAPI()\n",
    "        \n",
    "        # Load trained model\n",
    "        self.model = tf.keras.models.load_model(f\"{model_path}.h5\")\n",
    "        self.scaler_features = joblib.load(f\"{model_path}_scaler_features.pkl\")\n",
    "        self.scaler_target = joblib.load(f\"{model_path}_scaler_target.pkl\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open(f\"{model_path}_metadata.json\", 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        self.sequence_length = metadata['sequence_length']\n",
    "        self.feature_columns = metadata['feature_columns']\n",
    "        \n",
    "        logger.info(f\"Loaded SurgeNN model: {model_path}\")\n",
    "    \n",
    "    def predict_storm_surge(self, latitude: float, longitude: float,\n",
    "                           station_id: Optional[str] = None) -> StormSurgePrediction:\n",
    "        \"\"\"Make real-time storm surge prediction\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Get weather data\n",
    "            logger.info(f\"Fetching weather data for {latitude:.3f}, {longitude:.3f}\")\n",
    "            weather_data = self.weather_api.get_weather_data(\n",
    "                latitude, longitude, hours_back=self.sequence_length + 2\n",
    "            )\n",
    "            \n",
    "            if len(weather_data) < self.sequence_length:\n",
    "                raise ValueError(f\"Insufficient weather data: {len(weather_data)} hours\")\n",
    "            \n",
    "            # Get recent sequence for prediction\n",
    "            recent_data = weather_data[self.feature_columns].tail(self.sequence_length)\n",
    "            X = recent_data.values.reshape(1, self.sequence_length, -1)\n",
    "            \n",
    "            # Scale features\n",
    "            X_scaled = self.scaler_features.transform(X.reshape(-1, X.shape[-1]))\n",
    "            X_scaled = X_scaled.reshape(X.shape)\n",
    "            \n",
    "            # Make prediction\n",
    "            y_pred_scaled = self.model.predict(X_scaled, verbose=0)\n",
    "            surge_height = self.scaler_target.inverse_transform(y_pred_scaled)[0, 0]\n",
    "            \n",
    "            # Calculate confidence (simple approach)\n",
    "            confidence = min(0.95, max(0.5, 0.85 - abs(surge_height) * 0.1))\n",
    "            \n",
    "            # Determine alert level\n",
    "            alert_level = self._get_alert_level(surge_height)\n",
    "            \n",
    "            # Get tide data if available\n",
    "            current_tide = 0.0\n",
    "            if station_id:\n",
    "                try:\n",
    "                    tide_data = self.tide_api.get_tide_data(station_id, hours_back=2)\n",
    "                    if not tide_data.empty:\n",
    "                        current_tide = tide_data['water_level'].iloc[-1]\n",
    "                        logger.info(f\"Current tide level: {current_tide:.2f}m\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not get tide data: {e}\")\n",
    "            \n",
    "            # Adjust prediction with current tide\n",
    "            total_water_level = surge_height + current_tide\n",
    "            \n",
    "            prediction = StormSurgePrediction(\n",
    "                prediction_time=datetime.now(),\n",
    "                surge_height=total_water_level,\n",
    "                alert_level=alert_level,\n",
    "                confidence=confidence,\n",
    "                location=self.location_name\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Prediction: {surge_height:.3f}m surge, {total_water_level:.3f}m total\")\n",
    "            return prediction\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _get_alert_level(self, surge_height: float) -> str:\n",
    "        \"\"\"Determine alert level based on surge height\"\"\"\n",
    "        \n",
    "        if surge_height >= 2.5:\n",
    "            return \"MAJOR_FLOOD\"\n",
    "        elif surge_height >= 2.0:\n",
    "            return \"MODERATE_FLOOD\"\n",
    "        elif surge_height >= 1.5:\n",
    "            return \"MINOR_FLOOD\"\n",
    "        elif surge_height >= 1.0:\n",
    "            return \"ELEVATED\"\n",
    "        else:\n",
    "            return \"NORMAL\"\n",
    "    \n",
    "    def get_detailed_forecast(self, latitude: float, longitude: float,\n",
    "                            station_id: Optional[str] = None, \n",
    "                            hours_ahead: int = 12) -> List[Dict]:\n",
    "        \"\"\"Get detailed hourly forecast\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Get current weather data\n",
    "            weather_data = self.weather_api.get_weather_data(\n",
    "                latitude, longitude, hours_back=self.sequence_length\n",
    "            )\n",
    "            \n",
    "            forecast = []\n",
    "            \n",
    "            # For simplicity, we'll make predictions assuming weather continues\n",
    "            # In production, you'd use weather forecast data\n",
    "            last_weather = weather_data.iloc[-1].copy()\n",
    "            \n",
    "            for hour in range(1, hours_ahead + 1):\n",
    "                # Create future timestamp\n",
    "                future_time = datetime.now() + timedelta(hours=hour)\n",
    "                \n",
    "                # Update time features\n",
    "                last_weather['hour'] = future_time.hour\n",
    "                last_weather['day_of_year'] = future_time.timetuple().tm_yday\n",
    "                \n",
    "                # Add some realistic variation\n",
    "                last_weather['wind_speed'] *= (1 + np.random.normal(0, 0.1))\n",
    "                last_weather['wind_direction'] += np.random.normal(0, 5)\n",
    "                last_weather['pressure'] += np.random.normal(0, 200)\n",
    "                \n",
    "                # Recalculate wind components\n",
    "                wind_rad = last_weather['wind_direction'] * np.pi / 180\n",
    "                last_weather['wind_u'] = -last_weather['wind_speed'] * np.sin(wind_rad)\n",
    "                last_weather['wind_v'] = -last_weather['wind_speed'] * np.cos(wind_rad)\n",
    "                \n",
    "                # Append to weather data for sequence\n",
    "                new_row = pd.DataFrame([last_weather])\n",
    "                weather_data = pd.concat([weather_data, new_row], ignore_index=True)\n",
    "                \n",
    "                # Make prediction with updated sequence\n",
    "                recent_data = weather_data[self.feature_columns].tail(self.sequence_length)\n",
    "                X = recent_data.values.reshape(1, self.sequence_length, -1)\n",
    "                X_scaled = self.scaler_features.transform(X.reshape(-1, X.shape[-1]))\n",
    "                X_scaled = X_scaled.reshape(X.shape)\n",
    "                \n",
    "                y_pred_scaled = self.model.predict(X_scaled, verbose=0)\n",
    "                surge_height = self.scaler_target.inverse_transform(y_pred_scaled)[0, 0]\n",
    "                \n",
    "                forecast.append({\n",
    "                    'forecast_time': future_time,\n",
    "                    'hours_ahead': hour,\n",
    "                    'surge_height': surge_height,\n",
    "                    'alert_level': self._get_alert_level(surge_height),\n",
    "                    'wind_speed': last_weather['wind_speed'],\n",
    "                    'wind_direction': last_weather['wind_direction'],\n",
    "                    'pressure': last_weather['pressure'] / 100  # Convert to hPa\n",
    "                })\n",
    "            \n",
    "            return forecast\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Detailed forecast failed: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edde18da-5fc2-4d1d-8ab9-a77a07b3333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlertSystem:\n",
    "    \"\"\"Storm surge alert and notification system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alert_history = []\n",
    "        \n",
    "    def process_prediction(self, prediction: StormSurgePrediction) -> Dict:\n",
    "        \"\"\"Process prediction and generate alerts\"\"\"\n",
    "        \n",
    "        alert_info = {\n",
    "            'timestamp': prediction.prediction_time,\n",
    "            'location': prediction.location,\n",
    "            'surge_height': prediction.surge_height,\n",
    "            'alert_level': prediction.alert_level,\n",
    "            'confidence': prediction.confidence,\n",
    "            'message': self._generate_alert_message(prediction),\n",
    "            'color_code': self._get_color_code(prediction.alert_level),\n",
    "            'priority': self._get_priority(prediction.alert_level)\n",
    "        }\n",
    "        \n",
    "        # Store alert history\n",
    "        self.alert_history.append(alert_info)\n",
    "        \n",
    "        # Keep only last 100 alerts\n",
    "        if len(self.alert_history) > 100:\n",
    "            self.alert_history = self.alert_history[-100:]\n",
    "        \n",
    "        return alert_info\n",
    "    \n",
    "    def _generate_alert_message(self, prediction: StormSurgePrediction) -> str:\n",
    "        \"\"\"Generate human-readable alert message\"\"\"\n",
    "        \n",
    "        level_messages = {\n",
    "            'MAJOR_FLOOD': f\"🔴 MAJOR FLOOD WARNING: Storm surge of {prediction.surge_height:.1f}m predicted. Immediate evacuation may be required.\",\n",
    "            'MODERATE_FLOOD': f\"🟠 MODERATE FLOOD WARNING: Storm surge of {prediction.surge_height:.1f}m expected. Prepare for coastal flooding.\",\n",
    "            'MINOR_FLOOD': f\"🟡 MINOR FLOOD WATCH: Storm surge of {prediction.surge_height:.1f}m possible. Monitor conditions closely.\",\n",
    "            'ELEVATED': f\"🔵 ELEVATED CONDITIONS: Storm surge of {prediction.surge_height:.1f}m forecast. Stay informed.\",\n",
    "            'NORMAL': f\"🟢 NORMAL CONDITIONS: Storm surge of {prediction.surge_height:.1f}m. No immediate concerns.\"\n",
    "        }\n",
    "        \n",
    "        return level_messages.get(prediction.alert_level, f\"Storm surge: {prediction.surge_height:.1f}m\")\n",
    "    \n",
    "    def _get_color_code(self, alert_level: str) -> str:\n",
    "        \"\"\"Get color code for alert level\"\"\"\n",
    "        colors = {\n",
    "            'MAJOR_FLOOD': '#FF0000',      # Red\n",
    "            'MODERATE_FLOOD': '#FF8C00',   # Orange\n",
    "            'MINOR_FLOOD': '#FFD700',      # Gold\n",
    "            'ELEVATED': '#4169E1',         # Blue\n",
    "            'NORMAL': '#32CD32'            # Green\n",
    "        }\n",
    "        return colors.get(alert_level, '#808080')\n",
    "    \n",
    "    def _get_priority(self, alert_level: str) -> int:\n",
    "        \"\"\"Get numerical priority for alert level\"\"\"\n",
    "        priorities = {\n",
    "            'MAJOR_FLOOD': 5,\n",
    "            'MODERATE_FLOOD': 4,\n",
    "            'MINOR_FLOOD': 3,\n",
    "            'ELEVATED': 2,\n",
    "            'NORMAL': 1\n",
    "        }\n",
    "        return priorities.get(alert_level, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069fe7c5-1e88-41c1-a967-0422bff6705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REAL-TIME STORM SURGE PREDICTION DEMO\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not locate function 'mse'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'keras.metrics', 'class_name': 'function', 'config': 'mse', 'registered_name': 'mse'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 115\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMonitoring stopped by user\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[43mdemo_real_time_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mdemo_real_time_system\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize system (use your trained model path)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[43mRealTimeSurgePredictor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msurgenn_synthetic_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDemo Coastal Station\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     alert_system \u001b[38;5;241m=\u001b[39m AlertSystem()\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Test locations (you can change these)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mRealTimeSurgePredictor.__init__\u001b[1;34m(self, model_path, location_name)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtide_api \u001b[38;5;241m=\u001b[39m TideGaugeAPI()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load trained model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_features \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_scaler_features.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_target \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_scaler_target.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:194\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    188\u001b[0m         filepath,\n\u001b[0;32m    189\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    191\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    192\u001b[0m     )\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:155\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    151\u001b[0m training_config \u001b[38;5;241m=\u001b[39m json_utils\u001b[38;5;241m.\u001b[39mdecode(training_config)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Compile model.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43msaving_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args_from_training_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m )\n\u001b[0;32m    159\u001b[0m saving_utils\u001b[38;5;241m.\u001b[39mtry_build_compiled_arguments(model)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Set optimizer weights.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:143\u001b[0m, in \u001b[0;36mcompile_args_from_training_config\u001b[1;34m(training_config, custom_objects)\u001b[0m\n\u001b[0;32m    141\u001b[0m loss_config \u001b[38;5;241m=\u001b[39m training_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43m_deserialize_nested_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# Ensure backwards compatibility for losses in legacy H5 files\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m _resolve_compile_arguments_compat(loss, loss_config, losses)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py:202\u001b[0m, in \u001b[0;36m_deserialize_nested_config\u001b[1;34m(deserialize_fn, config)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_single_object(config):\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    205\u001b[0m         k: _deserialize_nested_config(deserialize_fn, v)\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    207\u001b[0m     }\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\losses\\__init__.py:149\u001b[0m, in \u001b[0;36mdeserialize\u001b[1;34m(name, custom_objects)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.losses.deserialize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeserialize\u001b[39m(name, custom_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserializes a serialized loss class/function instance.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        A Keras `Loss` instance or a loss function.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_OBJECTS_DICT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:575\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_objects[config], types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m--> 575\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m                \u001b[49m\u001b[43mserialize_with_public_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_module_name\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m deserialize_keras_object(\n\u001b[0;32m    582\u001b[0m             serialize_with_public_class(\n\u001b[0;32m    583\u001b[0m                 module_objects[config], inner_config\u001b[38;5;241m=\u001b[39minner_config\n\u001b[0;32m    584\u001b[0m             ),\n\u001b[0;32m    585\u001b[0m             custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    586\u001b[0m         )\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PLAIN_TYPES):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:678\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    677\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m inner_config\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;66;03m# Below, handling of all classes.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;66;03m# First, is it a shared object?\u001b[39;00m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared_object_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:812\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[1;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    810\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 812\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure custom classes are decorated with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not locate function 'mse'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'keras.metrics', 'class_name': 'function', 'config': 'mse', 'registered_name': 'mse'}"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage and testing\n",
    "def demo_real_time_system():\n",
    "    \"\"\"Demonstrate the real-time storm surge prediction system\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"REAL-TIME STORM SURGE PREDICTION DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize system (use your trained model path)\n",
    "    try:\n",
    "        predictor = RealTimeSurgePredictor(\n",
    "            model_path=\"surgenn_synthetic_model\",\n",
    "            location_name=\"Demo Coastal Station\"\n",
    "        )\n",
    "        alert_system = AlertSystem()\n",
    "        \n",
    "        # Test locations (you can change these)\n",
    "        test_locations = [\n",
    "            {\"name\": \"New York Harbor\", \"lat\": 40.7128, \"lon\": -74.0060, \"station\": \"8518750\"},\n",
    "            {\"name\": \"San Francisco Bay\", \"lat\": 37.7749, \"lon\": -122.4194, \"station\": \"9414290\"},\n",
    "            {\"name\": \"Miami Beach\", \"lat\": 25.7617, \"lon\": -80.1918, \"station\": \"8723214\"}\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nTesting real-time predictions for {len(test_locations)} locations:\")\n",
    "        \n",
    "        for i, location in enumerate(test_locations, 1):\n",
    "            print(f\"\\n{i}. {location['name']} ({location['lat']:.3f}, {location['lon']:.3f})\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            try:\n",
    "                # Make prediction\n",
    "                prediction = predictor.predict_storm_surge(\n",
    "                    latitude=location['lat'],\n",
    "                    longitude=location['lon'],\n",
    "                    station_id=location['station']\n",
    "                )\n",
    "                \n",
    "                # Process alert\n",
    "                alert = alert_system.process_prediction(prediction)\n",
    "                \n",
    "                # Display results\n",
    "                print(f\"Prediction Time: {prediction.prediction_time.strftime('%Y-%m-%d %H:%M UTC')}\")\n",
    "                print(f\"Storm Surge:     {prediction.surge_height:.2f} meters\")\n",
    "                print(f\"Alert Level:     {prediction.alert_level}\")\n",
    "                print(f\"Confidence:      {prediction.confidence:.1%}\")\n",
    "                print(f\"Alert Message:   {alert['message']}\")\n",
    "                \n",
    "                # Get detailed forecast\n",
    "                print(f\"\\n6-Hour Detailed Forecast:\")\n",
    "                forecast = predictor.get_detailed_forecast(\n",
    "                    location['lat'], location['lon'], \n",
    "                    location['station'], hours_ahead=6\n",
    "                )\n",
    "                \n",
    "                for f in forecast:\n",
    "                    print(f\"  +{f['hours_ahead']:2d}h: {f['surge_height']:5.2f}m \"\n",
    "                          f\"({f['alert_level']}) - Wind: {f['wind_speed']:4.1f}m/s @ {f['wind_direction']:3.0f}°\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error: {e}\")\n",
    "                \n",
    "            time.sleep(1)  # Be nice to APIs\n",
    "        \n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"REAL-TIME SYSTEM DEMO COMPLETE!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nNext steps for your hackathon:\")\n",
    "        print(\"1. Set up automatic scheduling (every 15-30 minutes)\")\n",
    "        print(\"2. Build web dashboard to display results\")\n",
    "        print(\"3. Add SMS/email notifications\")\n",
    "        print(\"4. Deploy to cloud platform (AWS, Heroku, etc.)\")\n",
    "        print(\"5. Add more coastal locations\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ Model files not found. Please run the training script first!\")\n",
    "        print(\"Expected files:\")\n",
    "        print(\"  - surgenn_synthetic_model.h5\")\n",
    "        print(\"  - surgenn_synthetic_model_scaler_features.pkl\") \n",
    "        print(\"  - surgenn_synthetic_model_scaler_target.pkl\")\n",
    "        print(\"  - surgenn_synthetic_model_metadata.json\")\n",
    "\n",
    "# Continuous monitoring function\n",
    "def run_continuous_monitoring(predictor: RealTimeSurgePredictor, \n",
    "                             alert_system: AlertSystem,\n",
    "                             locations: List[Dict],\n",
    "                             interval_minutes: int = 30):\n",
    "    \"\"\"Run continuous storm surge monitoring\"\"\"\n",
    "    \n",
    "    print(f\"Starting continuous monitoring (every {interval_minutes} minutes)...\")\n",
    "    print(\"Press Ctrl+C to stop\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            for location in locations:\n",
    "                try:\n",
    "                    prediction = predictor.predict_storm_surge(\n",
    "                        location['lat'], location['lon'], location.get('station')\n",
    "                    )\n",
    "                    alert = alert_system.process_prediction(prediction)\n",
    "                    \n",
    "                    # Log high-priority alerts\n",
    "                    if alert['priority'] >= 3:\n",
    "                        print(f\"🚨 {location['name']}: {alert['message']}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Monitoring error for {location['name']}: {e}\")\n",
    "            \n",
    "            # Wait before next cycle\n",
    "            time.sleep(interval_minutes * 60)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nMonitoring stopped by user\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_real_time_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c9c4ae-117b-4657-a01e-eaa3791ef461",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ee'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# data_collector.py\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mee\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ee'"
     ]
    }
   ],
   "source": [
    "# data_collector.py\n",
    "import ee\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class SatelliteDataCollector:\n",
    "    def __init__(self):\n",
    "        # Initialize Google Earth Engine\n",
    "        try:\n",
    "            ee.Initialize()\n",
    "        except:\n",
    "            print(\"Please authenticate with Google Earth Engine first\")\n",
    "            ee.Authenticate()\n",
    "            ee.Initialize()\n",
    "    \n",
    "    def download_sentinel2_images(self, location, start_date, end_date, max_cloud=20):\n",
    "        \"\"\"Download Sentinel-2 images for specified location and date range\"\"\"\n",
    "        \n",
    "        # Define area of interest (AOI)\n",
    "        point = ee.Geometry.Point(location)\n",
    "        aoi = point.buffer(5000)  # 5km buffer\n",
    "        \n",
    "        # Filter Sentinel-2 collection\n",
    "        collection = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
    "            .filterBounds(aoi) \\\n",
    "            .filterDate(start_date, end_date) \\\n",
    "            .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', max_cloud))\n",
    "        \n",
    "        # Get the least cloudy image\n",
    "        image = collection.sort('CLOUDY_PIXEL_PERCENTAGE').first()\n",
    "        \n",
    "        # Export parameters\n",
    "        export_params = {\n",
    "            'image': image.select(['B4', 'B3', 'B2']),  # RGB bands\n",
    "            'description': f'coastal_image_{start_date}',\n",
    "            'folder': 'coastal_monitoring',\n",
    "            'scale': 10,\n",
    "            'region': aoi,\n",
    "            'maxPixels': 1e9\n",
    "        }\n",
    "        \n",
    "        # Start export task\n",
    "        task = ee.batch.Export.image.toDrive(**export_params)\n",
    "        task.start()\n",
    "        \n",
    "        return task\n",
    "    \n",
    "    def collect_sample_dataset(self):\n",
    "        \"\"\"Collect a sample dataset for multiple locations and time periods\"\"\"\n",
    "        \n",
    "        locations = {\n",
    "            'miami_beach': [25.7617, -80.1918],\n",
    "            'outer_banks': [35.5582, -75.4665],\n",
    "            'pacifica': [37.6138, -122.4869]\n",
    "        }\n",
    "        \n",
    "        time_periods = [\n",
    "            ('2020-01-01', '2020-12-31'),\n",
    "            ('2022-01-01', '2022-12-31'),\n",
    "            ('2024-01-01', '2024-12-31')\n",
    "        ]\n",
    "        \n",
    "        tasks = []\n",
    "        for location_name, coords in locations.items():\n",
    "            for start_date, end_date in time_periods:\n",
    "                task = self.download_sentinel2_images(coords, start_date, end_date)\n",
    "                tasks.append({\n",
    "                    'location': location_name,\n",
    "                    'date_range': f\"{start_date}_to_{end_date}\",\n",
    "                    'task': task\n",
    "                })\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "# Usage\n",
    "collector = SatelliteDataCollector()\n",
    "tasks = collector.collect_sample_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdbc8856-00f9-4892-89aa-65aa9846b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUICK MODEL FIX\n",
      "============================================================\n",
      "1. Checking files:\n",
      "   ✅ surgenn_synthetic_model.h5\n",
      "   ✅ surgenn_synthetic_model_scaler_features.pkl\n",
      "   ✅ surgenn_synthetic_model_scaler_target.pkl\n",
      "   ❌ surgenn_synthetic_model_scaler_metadata.json\n",
      "   ✅ surgenn_synthetic_model_metadata.json\n",
      "\n",
      "3. Testing model loading...\n",
      "   TensorFlow version: 2.18.0\n",
      "   ✅ Method 1 SUCCESS: Loaded with custom objects!\n",
      "   Model input shape: (None, 24, 8)\n",
      "   Model output shape: (None, 1)\n",
      "\n",
      "4. Testing complete system...\n",
      "   ✅ All support files loaded successfully!\n",
      "   Sequence length: 24\n",
      "   Features: 8\n",
      "\n",
      "5. Creating test prediction...\n",
      "   ✅ Test data created successfully!\n",
      "   Weather sequence shape: (24, 8)\n",
      "\n",
      "============================================================\n",
      "✅ ALL TESTS PASSED!\n",
      "============================================================\n",
      "\n",
      "Your model should now work. Try running your main script!\n",
      "\n",
      "If you still get errors, use the fixed model:\n",
      "Change model_path to: 'surgenn_synthetic_model_fixed'\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quick fix for your model file naming and MSE loading issue\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "def fix_model_files(model_path=\"surgenn_synthetic_model\"):\n",
    "    \"\"\"Fix the naming and loading issues\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"QUICK MODEL FIX\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Check if files exist\n",
    "    files_exist = {\n",
    "        'model': f\"{model_path}.h5\",\n",
    "        'scaler_features': f\"{model_path}_scaler_features.pkl\", \n",
    "        'scaler_target': f\"{model_path}_scaler_target.pkl\",\n",
    "        'metadata_old': f\"{model_path}_scaler_metadata.json\",\n",
    "        'metadata_new': f\"{model_path}_metadata.json\"\n",
    "    }\n",
    "    \n",
    "    print(\"1. Checking files:\")\n",
    "    for name, path in files_exist.items():\n",
    "        exists = \"✅\" if os.path.exists(path) else \"❌\"\n",
    "        print(f\"   {exists} {path}\")\n",
    "    \n",
    "    # 2. Copy metadata file to expected name\n",
    "    if os.path.exists(files_exist['metadata_old']) and not os.path.exists(files_exist['metadata_new']):\n",
    "        print(f\"\\n2. Copying metadata file...\")\n",
    "        shutil.copy2(files_exist['metadata_old'], files_exist['metadata_new'])\n",
    "        print(f\"   ✅ Copied to {files_exist['metadata_new']}\")\n",
    "    \n",
    "    # 3. Try loading the model\n",
    "    print(f\"\\n3. Testing model loading...\")\n",
    "    \n",
    "    try:\n",
    "        print(f\"   TensorFlow version: {tf.__version__}\")\n",
    "        \n",
    "        # Try method 1: Load with custom objects (TF 2.18+ compatible)\n",
    "        try:\n",
    "            # Try new path first (TF 2.16+)\n",
    "            mse_func = tf.keras.losses.mean_squared_error\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                # Try older path\n",
    "                mse_func = tf.keras.metrics.mean_squared_error\n",
    "            except AttributeError:\n",
    "                # Fallback to function\n",
    "                mse_func = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        custom_objects = {\n",
    "            'mse': mse_func,\n",
    "            'mean_squared_error': mse_func,\n",
    "            'MeanSquaredError': tf.keras.losses.MeanSquaredError,\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            model = tf.keras.models.load_model(files_exist['model'], custom_objects=custom_objects)\n",
    "            print(\"   ✅ Method 1 SUCCESS: Loaded with custom objects!\")\n",
    "            print(f\"   Model input shape: {model.input_shape}\")\n",
    "            print(f\"   Model output shape: {model.output_shape}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e1:\n",
    "            print(f\"   ❌ Method 1 failed: {e1}\")\n",
    "            \n",
    "            # Try method 2: Load without compiling\n",
    "            try:\n",
    "                model = tf.keras.models.load_model(files_exist['model'], compile=False)\n",
    "                model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "                print(\"   ✅ Method 2 SUCCESS: Loaded without compilation!\")\n",
    "                \n",
    "                # Save the fixed model\n",
    "                model.save(f\"{model_path}_fixed.h5\")\n",
    "                print(f\"   ✅ Saved fixed model as: {model_path}_fixed.h5\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"   ❌ Method 2 failed: {e2}\")\n",
    "                return False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ TensorFlow error: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_full_system(model_path=\"surgenn_synthetic_model\"):\n",
    "    \"\"\"Test the complete system with a simple prediction\"\"\"\n",
    "    \n",
    "    print(f\"\\n4. Testing complete system...\")\n",
    "    \n",
    "    try:\n",
    "        # Import your classes\n",
    "        import sys\n",
    "        import os\n",
    "        sys.path.append('.')\n",
    "        \n",
    "        # Test data loading\n",
    "        scaler_features = joblib.load(f\"{model_path}_scaler_features.pkl\")\n",
    "        scaler_target = joblib.load(f\"{model_path}_scaler_target.pkl\")\n",
    "        \n",
    "        with open(f\"{model_path}_metadata.json\", 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"   ✅ All support files loaded successfully!\")\n",
    "        print(f\"   Sequence length: {metadata.get('sequence_length', 'Unknown')}\")\n",
    "        print(f\"   Features: {len(metadata.get('feature_columns', []))}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ System test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_test_prediction():\n",
    "    \"\"\"Create a simple test prediction to verify everything works\"\"\"\n",
    "    \n",
    "    print(f\"\\n5. Creating test prediction...\")\n",
    "    \n",
    "    try:\n",
    "        import numpy as np\n",
    "        from datetime import datetime, timedelta\n",
    "        \n",
    "        # Create synthetic weather data for testing\n",
    "        sequence_length = 24\n",
    "        n_features = 8\n",
    "        \n",
    "        # Generate fake weather sequence\n",
    "        fake_weather = np.random.randn(sequence_length, n_features)\n",
    "        \n",
    "        # Simulate what your system would do\n",
    "        print(\"   ✅ Test data created successfully!\")\n",
    "        print(f\"   Weather sequence shape: {fake_weather.shape}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Test prediction failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    \n",
    "    model_path = \"surgenn_synthetic_model\"\n",
    "    \n",
    "    # Run all fixes\n",
    "    step1_ok = fix_model_files(model_path)\n",
    "    step2_ok = test_full_system(model_path)\n",
    "    step3_ok = create_test_prediction()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    if all([step1_ok, step2_ok, step3_ok]):\n",
    "        print(\"✅ ALL TESTS PASSED!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nYour model should now work. Try running your main script!\")\n",
    "        print(f\"\\nIf you still get errors, use the fixed model:\")\n",
    "        print(f\"Change model_path to: '{model_path}_fixed'\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ SOME TESTS FAILED\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        if not step1_ok:\n",
    "            print(\"- Model loading failed: Try updating TensorFlow or recreating the model\")\n",
    "        if not step2_ok:\n",
    "            print(\"- Support files missing: Check your file paths\")\n",
    "        if not step3_ok:\n",
    "            print(\"- System components missing: Check imports\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5c766-e0d1-48ea-8e5d-bc5f985310ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
